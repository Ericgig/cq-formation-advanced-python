{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Compiling Python Code\n",
    "---\n",
    "\n",
    "While it might seem unintuitive to talk about compiling an interpreted language, it is often an easy and overlooked solution to speeding up Python programs. The advantage of being an interpreted language is that most Python compilers do [Just-In-Time (JIT) compilation](https://en.wikipedia.org/wiki/Just-in-time_compilation), not unlike what [PyPy](http://pypy.org) is doing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cython\n",
    "\n",
    "We'll first have a look at the Cython (not to be confused with CPython). It has both pre-compilation and just-in-time compilation modes. We will use the former for now as it will help us understand what it's doing and make better use of it.\n",
    "\n",
    "It is important to note that Cython scripts use extensions to the language and as such, scripts must not end with the .py extension. The recommended extension is .pyx. In Jupyter notebooks we can load the Cython extension using %load_ext cython and mark code using %%cython."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "\n",
    "def approx_pi(intervals):\n",
    "    pi = 0.0\n",
    "    for i in range(intervals):\n",
    "        pi += (4 - 8 * (i % 2)) / (float)(2 * i + 1)\n",
    "    return pi\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 2:\n",
    "        print >> sys.stderr, \"usage: {0} <intervals>\".format(sys.argv[0])\n",
    "        sys.exit(1)\n",
    "\n",
    "    t1 = time.clock()\n",
    "    pi = approx_pi(int(sys.argv[1]))\n",
    "    t2 = time.clock()\n",
    "    print(\"PI is approximately %.16f, Error is %.16f\"%(pi, abs(pi - math.pi)))\n",
    "    print(\"Time = %.16f sec\\n\"%(t2 - t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define compilation steps, we must create a compilation script, written in Python. A simple one, found in the setup_cython.py file, would look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~ {.python}\n",
    "from distutils.core import setup\n",
    "from Cython.Build import cythonize\n",
    "setup(ext_modules = cythonize(\"*.pyx\"))\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to proceed with the compilation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~ {.input}\n",
    "$ python setup_cython.py build_ext --inplace\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After some compilation steps involving your C compiler (GCC, clang, icc, ...), you will get, on Unix platforms, a shared library named approx_pi_cython.so. This is very different than what we did with PyPy in that this is not immediately executable: it's only a library exposing functions so our main timing code cannot be executed. In Jupyter the %%cython takes care of all these mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit approx_pi(100000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we are not even twice as fast as our original Python code under CPython. To see why, we have to look at the annotated code, which shows an analysis of the compiled code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython --annotate\n",
    "def approx_pi(intervals):\n",
    "    pi = 0.0\n",
    "    for i in range(intervals):\n",
    "        pi += (4 - 8 * (i % 2)) / (float)(2 * i + 1)\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is only a snippet of the entire code but it's enough to understand what's going on.\n",
    "First, you'll notice you have a C comment with an arrow pointing to the line the next code refers to. This is helpful to know how a line or chunk of code has been translated to C.\n",
    "Second, we notice that our pi variable is not a double native type, as we would expect, but a Python object. That means every interaction with that variable cannot be native C code and must go back inside the Python VM, as seen in this snippet:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~ {.c}\n",
    "...\n",
    "    __pyx_t_5 = PyNumber_Multiply(__pyx_int_8, __pyx_t_2); ...\n",
    "...\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So even for basic arithmetic operations like multiplications, Python is involved. Going back and forth between C/Python that way explains why we don't get really better performance.\n",
    "But there is a way to help the Cython compiler and give it hint about data types. This is where we begin using language extensions, as in the approx_pi_cython2.pyx file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%cython \n",
    "def approx_pi(int intervals):\n",
    "    cdef double pi\n",
    "    cdef int i\n",
    "    pi = 0.0\n",
    "    for i in range(intervals):\n",
    "        pi += (4 - 8 * (i % 2)) / (float)(2 * i + 1)\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we did was add types to the input parameter (int), as well the two local variable pi and i (cdef double). Let's compile and run it to compare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time approx_pi(100000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now on par with the PyPy interpreter. One could argue that using PyPy is easier than compiling with Cython and they would have a point: PyPy doesn't require a C compiler nor a setup script to work. However, Cython will integrate with other C extensions. Let's try to do better with Cython by looking again at the generated C code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython --annotate\n",
    "def approx_pi(int intervals):\n",
    "    cdef double pi\n",
    "    cdef int i\n",
    "    pi = 0.0\n",
    "    for i in range(intervals):\n",
    "        pi += (4 - 8 * (i % 2)) / (float)(2 * i + 1)\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks almost right. Our variables are now native types (double and int). The only thing left is this call to __Pyx_mod_long instead of the (way faster) C modulo operator (%). This is done mainly because of different behavior when using negative numbers. In C, -1%10 == -1 and in Python, -1%10 == 9. Since we know we won't have any negative numbers going from 0 to intervals-1, we can safely tell the Cython compiler to use the native modulo operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython --annotate\n",
    "#cython:cdivision=True\n",
    "\n",
    "def approx_pi(int intervals):\n",
    "    cdef double pi\n",
    "    cdef int i\n",
    "    pi = 0.0\n",
    "    for i in range(intervals):\n",
    "        pi += (4 - 8 * (i % 2)) / (float)(2 * i + 1)\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython --annotate\n",
    "#cython:cdivision=True\n",
    "\n",
    "def approx_pi(int intervals):\n",
    "    cdef double pi\n",
    "    cdef int i\n",
    "    pi = 0.0\n",
    "    for i in range(intervals/2):\n",
    "        pi += 4 / (float)(4 * i + 1)\n",
    "        pi -= 4 / (float)(4 * i + 3)\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the #cython compiler directive must be at the very beginning of your pyx file. To access the list of valid compiler directives, head over to the [Cython documentation page](http://docs.cython.org/src/reference/compilation.html#compiler-directives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time approx_pi(100000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now as fast as our first C code. I will let you have a look for yourself at the generated C code to confirm that the C modulo operator was indeed used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> ## Data Processing {.challenge}\n",
    ">\n",
    "> Try optimizing the exercices/process_data.py script using the Cython compiler. What speedup can you achieve?\n",
    ">\n",
    "> __Tip__: Before running the script, make a copy, generate a random sample and work on the pyx file:\n",
    ">\n",
    "> ~~~ {.input}\n",
    "> cd exercices/ && python gen_inputs.py && cp process_data.py process_data.pyx\n",
    "> ~~~\n",
    ">\n",
    "> A possible solution can be found in the solutions/process_data_cython.py file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../exercices/process_data.py\n",
    "from __future__ import division\n",
    "\n",
    "def read_data():\n",
    "    data = []\n",
    "    fp = open(\"inputs.dat\", \"r\")\n",
    "\n",
    "    line = 1\n",
    "    while line:\n",
    "        line = fp.readline()\n",
    "        if line:\n",
    "            row = []\n",
    "            for elem in line.split(','):\n",
    "                elem = elem.strip()\n",
    "                if elem:\n",
    "                    row.append(float(elem))\n",
    "            data.append(row)\n",
    "        \n",
    "    fp.close()\n",
    "    return data\n",
    "\n",
    "def process_A(data):\n",
    "    \"\"\"\n",
    "    Return a new matrix of the same shape as data, with each original\n",
    "    element squared by it's transposition equivalent.\n",
    "\n",
    "    result[i][j] = data[i][j] ** data[j][i]\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for i in range(len(data)):\n",
    "        row = []\n",
    "        for j in range(len(data[i])):\n",
    "            row.append(data[i][j] ** data[j][i])\n",
    "        result.append(row)\n",
    "    return result\n",
    "\n",
    "def process_B(m1, m2):\n",
    "    \"\"\"\n",
    "    Return the sum of the difference between each corresponding\n",
    "    elements of two square matrices.\n",
    "\n",
    "    diff = (m2[0][0] - m1[0][0]) + (m2[0][1] - m1[0][1]) + ...\n",
    "    \"\"\"\n",
    "\n",
    "    diff = 0.\n",
    "    for i in range(len(m1)):\n",
    "        for j in range(len(m1[i])):\n",
    "            diff += m2[i][j] - m1[i][j]\n",
    "    return diff\n",
    "\n",
    "def main():\n",
    "    data = read_data()\n",
    "    result_1 = process_A(data)\n",
    "    print \"Difference is: \", process_B(data, result_1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numba\n",
    "\n",
    "Another option for JIT compiling is the [Numba project](http://numba.pydata.org/). The Numba compiler is provided by Continuum Analytics, which also distribute the Anaconda Python distribution. In its simpler form, you only need to add the @jit annotation to the code you want to speed up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "import numpy\n",
    "\n",
    "@jit\n",
    "def approx_pi(intervals):\n",
    "    pi = 0.0\n",
    "    for i in range(intervals):\n",
    "        pi += (4 - 8 * (i % 2)) / (float)(2 * i + 1)\n",
    "    return pi\n",
    "\n",
    "%timeit approx_pi(100000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would result in the following execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit approx_pi(100000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, keep in mind that, altough it might be worth a try, applying the Numba @jit annotation doesn't provide much more gain when your code already uses Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def approx_pi(intervals):\n",
    "    pi1 = 4/numpy.arange(1, intervals, 4)\n",
    "    pi2 = -4/numpy.arange(3, intervals, 4)\n",
    "    return numpy.sum(pi1) + numpy.sum(pi2)\n",
    "\n",
    "%time approx_pi(100000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
